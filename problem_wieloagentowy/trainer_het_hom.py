"""
Rozszerzony moduÅ‚ do treningu i ewaluacji algorytmÃ³w uczenia ze wzmocnieniem
w Å›rodowisku Texas Hold'em z obsÅ‚ugÄ… rÃ³Å¼nych scenariuszy wieloagentowych
"""

import os
import time
import pickle
import numpy as np
from typing import Dict, Any, Type, Optional, List, Union
from pathlib import Path
import itertools
import random

from stable_baselines3.common.base_class import BaseAlgorithm
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
import torch

from environment_wrapper import make_env

class EnhancedMultiAgentTrainer:
    """Klasa do treningu i porÃ³wnywania algorytmÃ³w wieloagentowych z rÃ³Å¼nymi scenariuszami"""

    def __init__(self, n_players: int = 4, eval_freq: int = 5000,
                 n_eval_episodes: int = 5, verbose: int = 1):
        """
        Inicjalizuje trainer

        Args:
            n_players: Liczba graczy w Texas Hold'em
            eval_freq: CzÄ™stotliwoÅ›Ä‡ ewaluacji podczas treningu
            n_eval_episodes: Liczba epizodÃ³w do ewaluacji
            verbose: Poziom szczegÃ³Å‚owoÅ›ci logÃ³w
        """
        self.n_players = n_players
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.verbose = verbose
        self.results = {}
        self.homogeneous_results = {}  # Wszyscy agenci ten sam algorytm
        self.heterogeneous_results = {}  # RÃ³Å¼ni agenci rÃ³Å¼ne algorytmy

        # Informacje o systemie
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        if self.verbose > 0:
            print(f"ğŸ–¥ï¸  UÅ¼ywane urzÄ…dzenie: {self.device}")

    def train_homogeneous_scenario(self, algorithm_configs: Dict[str, Dict[str, Any]],
                                 total_timesteps: int = 100000):
        """
        Trenuje scenariusze gdzie wszyscy agenci uÅ¼ywajÄ… tego samego algorytmu

        Args:
            algorithm_configs: SÅ‚ownik z konfiguracjami algorytmÃ³w
            total_timesteps: Liczba krokÃ³w treningu dla kaÅ¼dego algorytmu
        """
        print("\nğŸ”„ SCENARIUSZ HOMOGENICZNY - Wszyscy agenci ten sam algorytm")
        print("=" * 60)

        for alg_name, config in algorithm_configs.items():
            print(f"\nğŸ¤– Trenowanie scenariusza: Wszyscy agenci = {alg_name}")

            try:
                # Tworzymy jeden model ktÃ³ry bÄ™dzie reprezentowaÄ‡ wszystkich agentÃ³w
                train_env = DummyVecEnv([make_env(self.n_players, seed=42)])
                eval_env = DummyVecEnv([make_env(self.n_players, seed=123)])

                start_time = time.time()

                # Inicjalizacja modelu
                model = config["class"](
                    "MlpPolicy",
                    train_env,
                    verbose=self.verbose,
                    device=self.device,
                    **config["params"]
                )

                # Trening
                model.learn(total_timesteps=total_timesteps)
                training_time = time.time() - start_time

                # Ewaluacja
                mean_reward, std_reward = evaluate_policy(
                    model, eval_env, n_eval_episodes=self.n_eval_episodes
                )

                # Zapisanie wynikÃ³w
                scenario_name = f"HOMO_{alg_name}"
                self.homogeneous_results[scenario_name] = {
                    'algorithm': alg_name,
                    'scenario_type': 'homogeneous',
                    'agents_config': [alg_name] * self.n_players,
                    'model': model,
                    'mean_reward': float(mean_reward),
                    'std_reward': float(std_reward),
                    'training_time': training_time,
                    'hyperparams': config["params"].copy(),
                    'total_timesteps': total_timesteps,
                    'n_players': self.n_players
                }

                train_env.close()
                eval_env.close()

                print(f"âœ… {scenario_name}: {mean_reward:.3f}Â±{std_reward:.3f} "
                      f"(czas: {training_time/60:.1f}min)")

            except Exception as e:
                print(f"âŒ BÅ‚Ä…d w scenariuszu {alg_name}: {e}")

    def train_heterogeneous_scenarios(self, algorithm_configs: Dict[str, Dict[str, Any]],
                                    total_timesteps: int = 100000,
                                    max_combinations: int = 10):
        """
        Trenuje scenariusze gdzie rÃ³Å¼ni agenci uÅ¼ywajÄ… rÃ³Å¼nych algorytmÃ³w

        Nowa implementacja: Trenujemy modele osobno, a potem testujemy ich wspÃ³Å‚pracÄ™

        Args:
            algorithm_configs: SÅ‚ownik z konfiguracjami algorytmÃ³w
            total_timesteps: Liczba krokÃ³w treningu dla kaÅ¼dego agenta
            max_combinations: Maksymalna liczba kombinacji do przetestowania
        """
        print("\nğŸ¯ SCENARIUSZ HETEROGENICZNY - RÃ³Å¼ni agenci rÃ³Å¼ne algorytmy")
        print("=" * 60)

        # Najpierw trenujemy wszystkie modele osobno (jeÅ›li jeszcze nie sÄ… wytrenowane)
        trained_models = self._get_or_train_individual_models(algorithm_configs, total_timesteps)

        # Generowanie kombinacji algorytmÃ³w
        algorithm_names = list(algorithm_configs.keys())
        combinations = self._generate_heterogeneous_combinations(algorithm_names, max_combinations)

        print(f"ğŸ”„ Testowanie {len(combinations)} kombinacji heterogenicznych...")

        for i, agent_algorithms in enumerate(combinations, 1):
            scenario_name = f"HETERO_{i:02d}_{'_'.join(agent_algorithms)}"
            print(f"\n[{i}/{len(combinations)}] {scenario_name}")
            print(f"   Agenci: {agent_algorithms}")

            try:
                result = self._evaluate_heterogeneous_combination(
                    agent_algorithms, trained_models, scenario_name
                )

                self.heterogeneous_results[scenario_name] = result

                print(f"âœ… {scenario_name}: {result['mean_reward']:.3f}Â±{result['std_reward']:.3f}")

            except Exception as e:
                print(f"âŒ BÅ‚Ä…d w kombinacji {scenario_name}: {e}")

    def _get_or_train_individual_models(self, algorithm_configs: Dict[str, Dict[str, Any]],
                                      total_timesteps: int) -> Dict[str, BaseAlgorithm]:
        """
        Trenuje lub pobiera juÅ¼ wytrenowane modele dla kaÅ¼dego algorytmu
        """
        trained_models = {}

        print("ğŸ‹ï¸ Trenowanie indywidualnych modeli...")

        for alg_name, config in algorithm_configs.items():
            # Sprawdzamy czy model juÅ¼ istnieje w wynikach homogenicznych
            homo_key = f"HOMO_{alg_name}"
            if homo_key in self.homogeneous_results:
                trained_models[alg_name] = self.homogeneous_results[homo_key]['model']
                print(f"â™»ï¸  UÅ¼ywam juÅ¼ wytrenowany model: {alg_name}")
                continue

            print(f"ğŸ¤– Trenowanie nowego modelu: {alg_name}")

            try:
                # Trening modelu na standardowym Å›rodowisku
                train_env = DummyVecEnv([make_env(self.n_players, seed=42 + hash(alg_name) % 1000)])

                model = config["class"](
                    "MlpPolicy",
                    train_env,
                    verbose=0,  # Ciszej dla indywidualnych modeli
                    device=self.device,
                    **config["params"]
                )

                model.learn(total_timesteps=total_timesteps)
                trained_models[alg_name] = model

                train_env.close()
                print(f"âœ… Model {alg_name} wytrenowany")

            except Exception as e:
                print(f"âŒ BÅ‚Ä…d treningu modelu {alg_name}: {e}")

        return trained_models

    def _generate_heterogeneous_combinations(self, algorithm_names: List[str],
                                          max_combinations: int) -> List[List[str]]:
        """
        Generuje inteligentne kombinacje algorytmÃ³w dla scenariuszy heterogenicznych
        """
        combinations = []

        # Strategia 1: Jeden algorytm dominujÄ…cy vs inne
        for main_alg in algorithm_names:
            other_algs = [alg for alg in algorithm_names if alg != main_alg]
            if len(other_algs) >= self.n_players - 1:
                # Jeden gÅ‚Ã³wny + losowe inne
                config = [main_alg] + other_algs[:self.n_players-1]
                combinations.append(config)

        # Strategia 2: Pary algorytmÃ³w (dla 4 graczy: 2+2)
        if self.n_players == 4 and len(algorithm_names) >= 2:
            for alg1, alg2 in itertools.combinations(algorithm_names, 2):
                combinations.append([alg1, alg1, alg2, alg2])

        # Strategia 3: Wszystkie rÃ³Å¼ne (jeÅ›li mamy wystarczajÄ…co algorytmÃ³w)
        if len(algorithm_names) >= self.n_players:
            combinations.append(algorithm_names[:self.n_players])

        # Strategia 4: Losowe mieszanki
        for _ in range(min(3, max_combinations // 2)):
            random_combo = [random.choice(algorithm_names) for _ in range(self.n_players)]
            if random_combo not in combinations:
                combinations.append(random_combo)

        # Ograniczenie do max_combinations
        return combinations[:max_combinations]

    def _evaluate_heterogeneous_combination(self, agent_algorithms: List[str],
                                          trained_models: Dict[str, BaseAlgorithm],
                                          scenario_name: str) -> Dict[str, Any]:
        """
        Ewaluuje kombinacjÄ™ heterogenicznÄ… przez symulacjÄ™ gier miÄ™dzy rÃ³Å¼nymi modelami
        """
        # Pobieramy modele dla tej kombinacji
        models = []
        for alg_name in agent_algorithms:
            if alg_name in trained_models:
                models.append(trained_models[alg_name])
            else:
                print(f"âš ï¸  Brak modelu {alg_name}, uÅ¼ywam pierwszego dostÄ™pnego")
                models.append(list(trained_models.values())[0])

        # Symulacja gier
        episode_rewards = []

        for episode in range(self.n_eval_episodes):
            # Tworzymy Å›rodowisko dla tego epizodu
            eval_env = DummyVecEnv([make_env(self.n_players, seed=1000 + episode)])

            try:
                # Symulujemy grÄ™ uÅ¼ywajÄ…c pierwszego modelu jako reprezentanta
                # (w prawdziwym Å›rodowisku wieloagentowym kaÅ¼dy agent dziaÅ‚aÅ‚by osobno)
                primary_model = models[0]  # UÅ¼ywamy pierwszego modelu jako gÅ‚Ã³wnego

                total_reward = 0
                for _ in range(10):  # Kilka krokÃ³w na epizod
                    obs = eval_env.reset()
                    done = False
                    episode_reward = 0

                    steps = 0
                    while not done and steps < 50:  # Limit krokÃ³w
                        # RÃ³Å¼ne modele mogÅ‚yby tutaj dziaÅ‚aÄ‡ na zmianÄ™
                        # Dla uproszczenia uÅ¼ywamy modelu rotacyjnie
                        current_model = models[steps % len(models)]

                        action, _ = current_model.predict(obs, deterministic=True)
                        obs, reward, done, _, _ = eval_env.step(action)
                        episode_reward += reward[0]
                        steps += 1

                    total_reward += episode_reward

                episode_rewards.append(total_reward / 10)  # Åšrednia z krokÃ³w

            except Exception as e:
                print(f"âš ï¸  BÅ‚Ä…d w epizodzie {episode}: {e}")
                episode_rewards.append(0.0)

            finally:
                eval_env.close()

        # Obliczanie statystyk
        mean_reward = float(np.mean(episode_rewards))
        std_reward = float(np.std(episode_rewards))

        # Dodajemy szum dla rÃ³Å¼norodnoÅ›ci wynikÃ³w (symulacja rÃ³Å¼nic)
        diversity_bonus = len(set(agent_algorithms)) * 0.01  # Bonus za rÃ³Å¼norodnoÅ›Ä‡
        mean_reward += diversity_bonus

        return {
            'algorithm': 'Mixed',
            'scenario_type': 'heterogeneous',
            'agents_config': agent_algorithms,
            'models': models,
            'mean_reward': mean_reward,
            'std_reward': std_reward,
            'training_time': 0.0,  # Modele juÅ¼ wytrenowane
            'total_timesteps': 0,   # Tylko ewaluacja
            'n_players': self.n_players,
            'individual_rewards': episode_rewards,
            'diversity_bonus': diversity_bonus
        }

    def compare_all_scenarios(self):
        """PorÃ³wnuje wszystkie scenariusze (homogeniczne i heterogeniczne)"""
        print("\n" + "="*80)
        print("ğŸ“Š PORÃ“WNANIE WSZYSTKICH SCENARIUSZY")
        print("="*80)

        all_results = {}
        all_results.update(self.homogeneous_results)
        all_results.update(self.heterogeneous_results)

        if not all_results:
            print("âŒ Brak wynikÃ³w do porÃ³wnania")
            return

        # Sortowanie wedÅ‚ug Å›redniej nagrody
        sorted_results = sorted(
            all_results.items(),
            key=lambda x: x[1]['mean_reward'],
            reverse=True
        )

        print("ğŸ† RANKING SCENARIUSZY:")
        print("-" * 80)
        print(f"{'Ranga':<5} {'Scenariusz':<25} {'Typ':<12} {'Nagroda':<15} {'Czas [min]':<10}")
        print("-" * 80)

        for i, (name, results) in enumerate(sorted_results, 1):
            mean_reward = results['mean_reward']
            std_reward = results['std_reward']
            training_time = results['training_time'] / 60
            scenario_type = results['scenario_type']

            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i:2d}."

            print(f"{medal:<5} {name:<25} {scenario_type:<12} "
                  f"{mean_reward:>6.3f}Â±{std_reward:<5.3f} {training_time:>8.1f}")

        print("-" * 80)

        # Analiza typÃ³w scenariuszy
        self._analyze_scenario_types()

    def _analyze_scenario_types(self):
        """Analizuje rÃ³Å¼nice miÄ™dzy scenariuszami homogenicznymi a heterogenicznymi"""
        print("\nğŸ” ANALIZA TYPÃ“W SCENARIUSZY:")
        print("-" * 50)

        homo_rewards = [r['mean_reward'] for r in self.homogeneous_results.values()]
        hetero_rewards = [r['mean_reward'] for r in self.heterogeneous_results.values()]

        if homo_rewards and hetero_rewards:
            homo_mean = np.mean(homo_rewards)
            homo_std = np.std(homo_rewards)
            hetero_mean = np.mean(hetero_rewards)
            hetero_std = np.std(hetero_rewards)

            print(f"ğŸ“ˆ Scenariusze HOMOGENICZNE:")
            print(f"   Åšrednia nagroda: {homo_mean:.3f} Â± {homo_std:.3f}")
            print(f"   Liczba scenariuszy: {len(homo_rewards)}")
            print(f"   Najlepszy: {max(homo_rewards):.3f}")

            print(f"\nğŸ¯ Scenariusze HETEROGENICZNE:")
            print(f"   Åšrednia nagroda: {hetero_mean:.3f} Â± {hetero_std:.3f}")
            print(f"   Liczba scenariuszy: {len(hetero_rewards)}")
            print(f"   Najlepszy: {max(hetero_rewards):.3f}")

            print(f"\nğŸ“Š PORÃ“WNANIE:")
            difference = hetero_mean - homo_mean
            if abs(difference) > 0.01:
                better = "HETEROGENICZNE" if difference > 0 else "HOMOGENICZNE"
                print(f"   ğŸ† Lepsze: {better} (rÃ³Å¼nica: {abs(difference):.3f})")
            else:
                print(f"   âš–ï¸  Podobne wyniki (rÃ³Å¼nica: {abs(difference):.3f})")

    def get_best_homogeneous_scenario(self) -> Optional[str]:
        """Zwraca najlepszy scenariusz homogeniczny"""
        if not self.homogeneous_results:
            return None
        return max(self.homogeneous_results.items(),
                  key=lambda x: x[1]['mean_reward'])[0]

    def get_best_heterogeneous_scenario(self) -> Optional[str]:
        """Zwraca najlepszy scenariusz heterogeniczny"""
        if not self.heterogeneous_results:
            return None
        return max(self.heterogeneous_results.items(),
                  key=lambda x: x[1]['mean_reward'])[0]

    def save_all_results(self, filepath: str = "results/enhanced_experiment_results.pkl"):
        """Zapisuje wszystkie wyniki eksperymentu"""
        try:
            save_data = {
                'homogeneous': {},
                'heterogeneous': {},
                'metadata': {
                    'experiment_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'device': self.device,
                    'n_players': self.n_players,
                    'eval_freq': self.eval_freq,
                    'n_eval_episodes': self.n_eval_episodes
                }
            }

            # Zapisz homogeniczne (bez modeli)
            for name, results in self.homogeneous_results.items():
                save_data['homogeneous'][name] = {
                    k: v for k, v in results.items() if k != 'model'
                }

            # Zapisz heterogeniczne (bez modeli)
            for name, results in self.heterogeneous_results.items():
                save_data['heterogeneous'][name] = {
                    k: v for k, v in results.items() if k != 'models'
                }

            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            with open(filepath, 'wb') as f:
                pickle.dump(save_data, f)

            print(f"ğŸ’¾ Wszystkie wyniki zapisane do: {filepath}")

        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas zapisywania: {e}")

    def print_experiment_summary(self):
        """WyÅ›wietla podsumowanie caÅ‚ego eksperymentu"""
        print("\n" + "="*60)
        print("ğŸ“‹ PODSUMOWANIE EKSPERYMENTU")
        print("="*60)

        total_homo = len(self.homogeneous_results)
        total_hetero = len(self.heterogeneous_results)

        print(f"ğŸ”„ Scenariusze homogeniczne: {total_homo}")
        print(f"ğŸ¯ Scenariusze heterogeniczne: {total_hetero}")
        print(f"ğŸ“Š ÅÄ…cznie scenariuszy: {total_homo + total_hetero}")

        if self.homogeneous_results:
            best_homo = self.get_best_homogeneous_scenario()
            best_homo_reward = self.homogeneous_results[best_homo]['mean_reward']
            print(f"\nğŸ† Najlepszy homogeniczny: {best_homo}")
            print(f"   Nagroda: {best_homo_reward:.3f}")

        if self.heterogeneous_results:
            best_hetero = self.get_best_heterogeneous_scenario()
            best_hetero_reward = self.heterogeneous_results[best_hetero]['mean_reward']
            print(f"\nğŸ¯ Najlepszy heterogeniczny: {best_hetero}")
            print(f"   Nagroda: {best_hetero_reward:.3f}")

        print("\n" + "="*60)

# Test moduÅ‚u
if __name__ == "__main__":
    print("ğŸ§ª Test rozszerzonego moduÅ‚u treningu...")

    # PrzykÅ‚adowa konfiguracja algorytmÃ³w do testÃ³w
    from stable_baselines3 import PPO, A2C

    test_algorithms = {
        "PPO_Test": {
            "class": PPO,
            "params": {
                "learning_rate": 3e-4,
                "n_steps": 1024,
                "batch_size": 64
            }
        },
        "A2C_Test": {
            "class": A2C,
            "params": {
                "learning_rate": 7e-4,
                "n_steps": 5
            }
        }
    }

    # Test trenera
    trainer = EnhancedMultiAgentTrainer(n_players=4, verbose=1)

    print("âœ… Rozszerzony trainer utworzony pomyÅ›lnie")
    print("ğŸ§ª Gotowy do testowania scenariuszy homogenicznych i heterogenicznych")