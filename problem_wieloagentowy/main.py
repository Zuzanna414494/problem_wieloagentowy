"""
Projekt 6: Problem wieloagentowy - Texas Hold'em
GÅ‚Ã³wny plik do uruchomienia eksperymentÃ³w z rozszerzonym trainerem
"""

import os
import sys
import time
from pathlib import Path

# Tworzenie struktury folderÃ³w
def setup_directories():
    """Tworzy niezbÄ™dne foldery"""
    directories = ['models', 'logs', 'results', 'plots']
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
    print("âœ… Struktura folderÃ³w utworzona")

def check_dependencies():
    """Sprawdza czy wszystkie biblioteki sÄ… zainstalowane"""
    required_packages = [
        'pettingzoo', 'stable_baselines3', 'matplotlib',
        'numpy', 'torch', 'gymnasium'
    ]

    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)

    if missing_packages:
        print("âŒ BrakujÄ…ce pakiety:")
        for package in missing_packages:
            print(f"   - {package}")
        print("\nğŸ“¦ Zainstaluj brakujÄ…ce pakiety:")
        print("pip install stable-baselines3[extra] pettingzoo[classic] matplotlib torch")
        return False

    print("âœ… Wszystkie zaleÅ¼noÅ›ci sÄ… zainstalowane")
    return True

def main():
    """GÅ‚Ã³wna funkcja projektu z rozszerzonym trainerem"""
    print("=" * 80)
    print("ğŸ¯ PROJEKT WIELOAGENTOWY - TEXAS HOLD'EM (ROZSZERZONY)")
    print("=" * 80)

    # Sprawdzenie zaleÅ¼noÅ›ci
    if not check_dependencies():
        return

    # Tworzenie struktury folderÃ³w
    setup_directories()

    # Import moduÅ‚Ã³w projektu
    try:
        from environment_wrapper import TexasHoldemWrapper
        from algorithms_config import AlgorithmsConfig
        from trainer import EnhancedMultiAgentTrainer
        from visualizer import ResultsVisualizer

        print("âœ… Wszystkie moduÅ‚y zaÅ‚adowane pomyÅ›lnie")
    except ImportError as e:
        print(f"âŒ BÅ‚Ä…d importu: {e}")
        print("ğŸ’¡ Upewnij siÄ™, Å¼e plik paste.py zawiera klasÄ™ EnhancedMultiAgentTrainer")
        return

    # Konfiguracja eksperymentu
    print("\nğŸ”§ Konfiguracja eksperymentu:")
    config = {
        'n_players': 4,              # 4 graczy w Texas Hold'em
        'total_timesteps': 20000,    # ZwiÄ™kszona liczba krokÃ³w dla lepszej jakoÅ›ci
        'eval_freq': 5000,           # CzÄ™stotliwoÅ›Ä‡ ewaluacji
        'n_eval_episodes': 10,       # WiÄ™cej epizodÃ³w dla stabilniejszych wynikÃ³w
        'max_combinations': 8        # Maksymalna liczba kombinacji heterogenicznych
    }

    for key, value in config.items():
        print(f"   {key}: {value}")

    # Inicjalizacja rozszerzonego trenera
    print("\nğŸ¤– Inicjalizacja rozszerzonego trenera...")
    trainer = EnhancedMultiAgentTrainer(
        n_players=config['n_players'],
        eval_freq=config['eval_freq'],
        n_eval_episodes=config['n_eval_episodes'],
        verbose=1
    )

    # Pobieranie konfiguracji algorytmÃ³w
    algorithms_config = AlgorithmsConfig.get_algorithms()
    print(f"ğŸ“‹ Skonfigurowane algorytmy: {list(algorithms_config.keys())}")

    # FAZA 1: Trening scenariuszy homogenicznych
    print("\n" + "="*60)
    print("ğŸ”„ FAZA 1: SCENARIUSZE HOMOGENICZNE")
    print("="*60)

    start_time = time.time()

    trainer.train_homogeneous_scenario(
        algorithm_configs=algorithms_config,
        total_timesteps=config['total_timesteps']
    )

    homo_time = time.time() - start_time
    print(f"â±ï¸  Czas treningu homogenicznych: {homo_time/60:.1f} minut")

    # FAZA 2: Trening scenariuszy heterogenicznych
    print("\n" + "="*60)
    print("ğŸ¯ FAZA 2: SCENARIUSZE HETEROGENICZNE")
    print("="*60)

    hetero_start_time = time.time()

    trainer.train_heterogeneous_scenarios(
        algorithm_configs=algorithms_config,
        total_timesteps=config['total_timesteps'] // 2,  # Mniej krokÃ³w dla kaÅ¼dego agenta
        max_combinations=config['max_combinations']
    )

    hetero_time = time.time() - hetero_start_time
    print(f"â±ï¸  Czas treningu heterogenicznych: {hetero_time/60:.1f} minut")

    total_time = time.time() - start_time
    print(f"â±ï¸  CaÅ‚kowity czas treningu: {total_time/60:.1f} minut")

    # FAZA 3: PorÃ³wnanie wszystkich scenariuszy
    print("\n" + "="*60)
    print("ğŸ“Š FAZA 3: ANALIZA WSZYSTKICH WYNIKÃ“W")
    print("="*60)

    trainer.compare_all_scenarios()

    # FAZA 4: Tworzenie wizualizacji (jeÅ›li visualizer obsÅ‚uguje nowy format)
    print("\nğŸ“ˆ Tworzenie wykresÃ³w...")
    try:
        # Przygotowanie danych dla wizualizatora
        combined_results = {}
        combined_results.update(trainer.homogeneous_results)
        combined_results.update(trainer.heterogeneous_results)

        if combined_results:
            visualizer = ResultsVisualizer(combined_results)
            visualizer.create_all_plots()
            print("âœ… Wykresy zostaÅ‚y wygenerowane")
        else:
            print("âš ï¸  Brak wynikÃ³w do wizualizacji")

    except Exception as e:
        print(f"âš ï¸  BÅ‚Ä…d podczas tworzenia wykresÃ³w: {e}")
        print("ğŸ’¡ Visualizer moÅ¼e wymagaÄ‡ aktualizacji dla nowego formatu danych")

    # FAZA 5: Zapisanie wynikÃ³w
    print("\nğŸ’¾ Zapisywanie wynikÃ³w...")
    trainer.save_all_results('results/enhanced_experiment_results.pkl')

    # FAZA 6: Podsumowanie koÅ„cowe
    print("\n" + "="*80)
    print("ğŸ‰ ROZSZERZONY EKSPERYMENT ZAKOÅƒCZONY POMYÅšLNIE!")
    print("="*80)

    # WyÅ›wietlenie podsumowania
    trainer.print_experiment_summary()

    print("\nğŸ“ Wygenerowane pliki:")
    print("   ğŸ“Š plots/ - Wykresy i wizualizacje")
    print("   ğŸ¤– models/ - Wytrenowane modele")
    print("   ğŸ“ logs/ - Logi treningu")
    print("   ğŸ’¾ results/enhanced_experiment_results.pkl - Rozszerzone dane eksperymentu")

    # Najlepsze scenariusze z kaÅ¼dej kategorii
    best_homo = trainer.get_best_homogeneous_scenario()
    best_hetero = trainer.get_best_heterogeneous_scenario()

    print("\nğŸ† NAJLEPSZE SCENARIUSZE:")
    print("-" * 50)

    if best_homo:
        homo_reward = trainer.homogeneous_results[best_homo]['mean_reward']
        print(f"ğŸ”„ Homogeniczny: {best_homo}")
        print(f"   Nagroda: {homo_reward:.3f}")
        print(f"   Agenci: {trainer.homogeneous_results[best_homo]['agents_config']}")

    if best_hetero:
        hetero_reward = trainer.heterogeneous_results[best_hetero]['mean_reward']
        print(f"ğŸ¯ Heterogeniczny: {best_hetero}")
        print(f"   Nagroda: {hetero_reward:.3f}")
        print(f"   Agenci: {trainer.heterogeneous_results[best_hetero]['agents_config']}")

    # OgÃ³lne wnioski
    print("\nğŸ“Š WNIOSKI:")
    print("-" * 30)

    total_scenarios = len(trainer.homogeneous_results) + len(trainer.heterogeneous_results)
    print(f"ğŸ”¢ Przetestowano {total_scenarios} rÃ³Å¼nych scenariuszy")

    if best_homo and best_hetero:
        homo_best_reward = trainer.homogeneous_results[best_homo]['mean_reward']
        hetero_best_reward = trainer.heterogeneous_results[best_hetero]['mean_reward']

        if hetero_best_reward > homo_best_reward:
            diff = hetero_best_reward - homo_best_reward
            print(f"ğŸ¯ Scenariusze heterogeniczne okazaÅ‚y siÄ™ lepsze o {diff:.3f}")
            print("ğŸ’¡ RÃ³Å¼norodnoÅ›Ä‡ algorytmÃ³w moÅ¼e przynosiÄ‡ korzyÅ›ci!")
        elif homo_best_reward > hetero_best_reward:
            diff = homo_best_reward - hetero_best_reward
            print(f"ğŸ”„ Scenariusze homogeniczne okazaÅ‚y siÄ™ lepsze o {diff:.3f}")
            print("ğŸ’¡ SpÃ³jnoÅ›Ä‡ strategii moÅ¼e byÄ‡ kluczowa!")
        else:
            print("âš–ï¸  Oba typy scenariuszy daÅ‚y podobne wyniki")

    print(f"\nâ±ï¸  CaÅ‚kowity czas eksperymentu: {total_time/60:.1f} minut")

if __name__ == "__main__":
    main()